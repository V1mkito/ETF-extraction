# LLM-Powered ETF Attribute Extraction

This project uses a Large Language Model (LLM) to extract **structured financial attributes** from ETF disclosure documents, including fact sheets, prospectuses, annual reports, and quarterly reports.

It takes **per-ETF PDF text JSON files** as input and outputs **FNDA-style structured JSON** with full source traceability.

---

## Overview

### What This Project Does

- Converts unstructured ETF disclosure text into structured, machine-readable data
- Extracts both **numeric** and **categorical** ETF attributes
- Enforces a strict financial data schema
- Preserves document-level and page-level provenance for every extracted value

---

## Workflow

1. **Input**
   - JSON files containing parsed text from ETF PDFs
   - Each JSON file represents one ETF
   - These files are typically generated by a prior PDF text-extraction step

2. **Processing**
   - Builds `SecuritiesInformation.source_documents` from the input JSON
   - Sends ETF disclosure text to an OpenAI model
   - Extracts attributes into an FNDA-style schema
   - Omits any attribute not explicitly present in the source text

3. **Output**
   - One structured JSON file per ETF
   - Output files preserve the original filename
   - All extracted values include source document and page number

---

## Output Schema (High Level)

Each output file contains a single JSON object:

```json
{
  "SecuritiesInformation": {
    "security_type": "etf",
    "security_name": "string",
    "security_ticker": "string",
    "source_documents": [...],
    "extracted_attributes": {
      "Fundamentals": {...},
      "PerformanceRiskAdjusted": {...},
      "LiquidityAndExpenses": {...},
      "BrokerageSpecificAttributes": {...}
    }
  }
}
